---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<!-- ææ¢“å˜‰å›¾ç‰‡è£…é¥°å…ƒç´  - ä»é¡µé¢å¼€å§‹æ˜¾ç¤ºï¼Œå†…å®¹ç»“æŸåæˆªæ–­ -->
<div class="lee-decorative-gallery">
    <div class="decorative-item" style="display: block; opacity: 1;">
        <img src="{{ '/images/lee_zii_jia/å¾®ä¿¡å›¾ç‰‡_20240916120048_605_8.jpg' | relative_url }}" alt="Lee Zii Jia" style="display: block; opacity: 1;">
    </div>
    <div class="decorative-item">
        <img src="{{ '/images/lee_zii_jia/å¾®ä¿¡å›¾ç‰‡_20260129165205_438_54.jpg' | relative_url }}" alt="Lee Zii Jia">
    </div>
    <div class="decorative-item">
        <img src="{{ '/images/lee_zii_jia/å¾®ä¿¡å›¾ç‰‡_20260129165206_439_54.jpg' | relative_url }}" alt="Lee Zii Jia">
    </div>
    <div class="decorative-item">
        <img src="{{ '/images/lee_zii_jia/å¾®ä¿¡å›¾ç‰‡_20260129165207_440_54.jpg' | relative_url }}" alt="Lee Zii Jia">
    </div>
    <div class="decorative-item">
        <img src="{{ '/images/lee_zii_jia/å¾®ä¿¡å›¾ç‰‡_20260129165208_441_54.jpg' | relative_url }}" alt="Lee Zii Jia">
    </div>
    <div class="decorative-item">
        <img src="{{ '/images/lee_zii_jia/å¾®ä¿¡å›¾ç‰‡_20260129165208_442_54.jpg' | relative_url }}" alt="Lee Zii Jia">
    </div>
    <div class="decorative-item">
        <img src="{{ '/images/lee_zii_jia/å¾®ä¿¡å›¾ç‰‡_20260129165209_443_54.jpg' | relative_url }}" alt="Lee Zii Jia">
    </div>
    <div class="decorative-item">
        <img src="{{ '/images/lee_zii_jia/å¾®ä¿¡å›¾ç‰‡_20260129165210_444_54.jpg' | relative_url }}" alt="Lee Zii Jia">
    </div>
    <div class="decorative-item">
        <img src="{{ '/images/lee_zii_jia/å¾®ä¿¡å›¾ç‰‡_20260129165211_445_54.jpg' | relative_url }}" alt="Lee Zii Jia">
    </div>
    <div class="decorative-item">
        <img src="{{ '/images/lee_zii_jia/å¾®ä¿¡å›¾ç‰‡_20260129165212_446_54.jpg' | relative_url }}" alt="Lee Zii Jia">
    </div>
</div>

Here is <strong>Songning Lai</strong>.( You can call me Sony. )

I received my undergraduate degree from the School of Information Science and Engineering(<a href="https://baike.baidu.com/item/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E5%B4%87%E6%96%B0%E5%AD%A6%E5%A0%82/20809738?fr=aladdin"><strong>Chongxin College</strong></a>), <a href="https://www.sdu.edu.cn/"><strong>Shandong University</strong></a> in China,supervised by Prof. <a href="https://faculty.sdu.edu.cn/liuzhi1/zh_CN/index.htm"><strong>Zhi Liu</strong></a>. I was also an RA at HKUST@AI Thrust&INFO Hub, supervised by Prof. <a href="https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/YUE-Yutao/yutaoyue"><strong>Yutao Yue</strong></a>. And now I am a Quant Researcher in <a href="https://www.joinquant.com"><strong>JoinQuant</strong></a>.

My primary research interest lies in the domain of <strong>Trustworthy AI</strong> <span class="icon-emoji">ğŸ¤–</span>, encompassing <strong>explainability</strong> <span class="icon-emoji">ğŸ”</span>, <strong>robustness</strong> <span class="icon-emoji">ğŸ›¡ï¸</span>, <strong>faithfulness</strong> <span class="icon-emoji">âœ…</span>, and <strong>safety</strong> <span class="icon-emoji">ğŸ”’</span> of AI. Specifically, I have focused extensively on <strong>Concept Bottleneck Models (CBMs)</strong> within the realm of explainability. My past research includes an investigation into the robustness and generalization of CBMs in unsupervised settings <a href="https://openreview.net/forum?id=rp0EdI8X4e">(ICLR 2024)</a>, application of CBMs in multimodal contexts for unsupervised tasks (Under Review), pioneering work on continual learning with CBMs <a href="https://arxiv.org/pdf/2411.17471">(ACM MM 2025)</a>, as well as the first exploration of CBMs in the context of security, particularly backdoor attacks <a href="https://arxiv.org/pdf/2410.04823">(Under review 1;</a><a href="https://arxiv.org/pdf/2411.16512"> Under review 2)</a>. Furthermore, my research has extended to applying CBMs in medical fields <a href="https://arxiv.org/abs/2410.21494">(NIPS 2024;</a> <a href="https://arxiv.org/pdf/2506.05286?">ECML 2025</a>) and autonomous driving applications <a href="https://arxiv.org/pdf/2409.10330">(ICRA 2025)</a>.

Beyond my work with CBMs, I have also explored issues related to robustness and faithfulness in time series (<a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-TW&user=gRXN-rMAAAAJ&sortby=pubdate&citation_for_view=gRXN-rMAAAAJ:blknAaTinKkC">ICML 2025</a>; <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-TW&user=gRXN-rMAAAAJ&citation_for_view=gRXN-rMAAAAJ:KlAtU1dfN6UC">ACM MM 2025</a>,<a href="https://arxiv.org/pdf/2503.19656">ICASSP 2026</a>), continual learning<a href="https://arxiv.org/pdf/2412.10834">(ACM MM25)</a> and explainability for LLM (<a href="https://arxiv.org/pdf/2510.07896?">ICLR 2026</a>). Prior to these endeavors, my research efforts were directed towards computer vision (<a href="https://www.sciencedirect.com/science/article/pii/S0262885623002081">Image and Vison Computing</a>; <a href="https://arxiv.org/pdf/2409.03192">ICASSP 2025</a>), multimodal sentiment analysis (<a href="https://arxiv.org/pdf/2305.08473">IJCNN 2024</a>; <a href="https://www.sciencedirect.com/science/article/abs/pii/S0141938223001968">Displays</a>), and community detection (<a href="https://arxiv.org/pdf/2309.11798">Neurocomputing</a>).



If you are interested in any aspect of me, I would love to chat and collaborate <span class="icon-emoji">ğŸ’¬</span>, please email me at - <em>lais0328eee@gmail.com</em> <span class="icon-emoji">ğŸ“§</span>.

<div class="section-divider"></div>

<span class='anchor' id='news'></span>
# <span data-typewriter>ğŸ”¥ News</span>

<div class="news-scroll-container">

<ul>
<li><em>01.2026</em>  ğŸ‰ Our paper "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall" has been accepted at <strong>ICLR 2026</strong>. (CCF None)!</li>
<li><em>01.2026</em>  ğŸ‰ Our paper "TOWARDS RELIABLE TIME SERIESFORECASTING UNDER FUTURE UNCERTAINTY: AMBIGUITY AND NOVELTY REJECTION MECHANISMS" has been accepted at <strong>ICASSP 2026</strong>. (CCF B)!</li>

<li><em>01.2026</em>  ğŸ‰ Our paper "Towards Better Evaluation Metrics for Text-to-Motion Generation" has been accepted at <strong>ACM TheWebConf 2026 (WWW2026) Workshop TIME</strong>!</li>
<li><em>01.2026</em>  ğŸ‰ Our paper "TPTD: A Tursted Privacy-Preserving Truth Discovery Scheme for Quality Enhancement in Team-based Mobile Crowd Sensing" has been accepted at Knowledge-Based Systems <strong>KBS</strong>. (IF: 7.2, JCR Q1, CCF C)!</li>
<li><em>12.2025</em>  ğŸ‰ Our paper "Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding" has been accepted at IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY <strong>TCSVT</strong>. (IF: 11.1, JCR Q1, CCF B)!</li>
<li><em>10.2025</em>: ğŸ‰ Our paper "Mimicking the Physicist's Eye : A VLM-centric Approach for Physics Formula Discovery" has been accepted at <strong>NeurIPS 2025 Workshop on Efficient Reasoning</strong> with <strong>spotlight</strong>!</li>
<li><em>10.2025</em>: ğŸ‰ Our paper "Orientation-Aware Detection System for Real-Time Monitoring of Cracks in Steel Structures" has been accepted at <strong>Expert Systems With Applications</strong> (JCR Q1, IF: 7.5)!</li>
<li><em>09.2025</em>ï¼šğŸ‰ Our paper "Boosting Expertise and Efficiencyin LLM:A Knowledge-Enhanced Framework for Construction Support" has been accepted at <strong>Alexandria Engineering Journal</strong> (JCR Q1, IF: 6.8)!</li>
<li><em>08.2025</em>: ğŸ‰ Our 3 papers have been accepted at <strong>ACM MM 2025 BNI Track</strong> (CCF A, oral, BNI Papers are considered outstanding ACM MM full papers, and accepted BNI papers will apear in the main proceedings)! ("Learning New Concepts, Remembering the Old: Continual Learning for Multimodal Concept Bottleneck Models"; "Physics-Informed Representation Alignment for Sparse Radio-Map Reconstruction"; "Can Audio Language Models Listen Between the Lines? A Study on Metaphorical Reasoning via Unspoken")</li>
<li><em>07.2025</em>: ğŸ‰ Our 4 papers have been accepted at <strong>ACM MM 2025</strong> (CCF A, oral)!("From Guesswork to Guarantee: Towards Faithful Multimedia Web Forecasting with TimeSieve"; "ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model"; "Text2Weight: Bridging Natural Language and Neural Network Weight Spaces"; "CFSSeg: Closed-Form Solution for Class-Incremental Semantic Segmentation of 2D Images and 3D Point Clouds")</li>
<li><em>07.2025</em>: ğŸ‰ Our paper "VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results" has been accepted at <strong>ICCV 2025 workshop VQualA</strong>!</li>
<li><em>07.2025</em>: ğŸ‰ Our paper "Generative Knowledge-Guided Review System for Construction Disclosure Documents" has been accepted at <strong>Advanced engineering informatics</strong> (JCR Q1, IF: 9.9)!</li>
<li><em>06.2025</em>: ğŸ‰ Our paper "Automated Detection of Complex Construction Scenes Using a Lightweight Transformer-based Method" has been accepted at <strong>Automation in Construction</strong> (JCR Q1, IF:9.6)!</li>
<li><em>05.2025</em>: ğŸ‰ Our paper "Stable Vision Concept Transformers for Medical Diagnosis" has been accepted at <strong>ECML-PKDD 2025</strong> (CCF B)!</li>
<li><em>04.2025:</em> ğŸ‰ Our paper "IMTS is Worth Time X Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction" has been accepted by <strong>ICML 2025</strong> (CCF A)!</li>
<li><em>04.2025:</em> ğŸ‰ Our paper "Class Incremental Semantic Segmentation Based on Linear Closed-form Solution" has been accepted by <strong>CVPR 2025 workshop BASE</strong>!</li>
<li><em>04.2025:</em> ğŸ‰ Our paper "Beyond Patterns: Harnessing Causal Logic for Autonomous Driving Trajectory Prediction" has been accepted by <strong>IJCAI 2025</strong> (CCF A)!</li>
<li><em>02.2025:</em> ğŸ‰ Our paper "Enhancing domain adaptation for plant diseases detection through Masked Image Consistency in Multi-Granularity Alignment" has been accepted by <strong>Expert Systems With Applications</strong> (JCR Q1, IF:8.4, CCF C).</li>
<li><em>01.2025:</em> ğŸ‰ Our paper "Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving" has been accepted by <strong>ICRA 2025</strong> (CCF B)!</li>
<li><em>12.2024:</em> ğŸ‰ Our paper "PEPL:Precision-Enhanced Pseudo-Labeling forFine-Grained lmage Classification inSemi-Supervised Learning" has been accepted at <strong>ICASSP 2025</strong> (CCF B)!</li>
<li><em>09.2024:</em> ğŸ‰ Our paper "Towards Multi-dimensional Explanation Alignment for Medical Classification" has been accepted by <strong>(NeurIPS 2024)</strong> (CCF A)!</li>
<li><em>07.2024:</em> ğŸ‰ Our paper "FTS: A Framework to Find a Faithful TimeSieve" has been accepted by <strong>IJCAI 2024 workshop</strong>.</li>
<li><em>06.2024:</em> ğŸ‰ Our paper "A Comprehensive Review of Community Detection in Graphs" has been accepted by <a href="https://arxiv.org/pdf/2309.11798">Neurocomputing</a>(JCR Q1; CCF C).</li>
<li><em>03.2024:</em> ğŸ‰I am awarded the honor of <strong>excellent graduate of Shandong Province</strong> and <strong>excellent graduate of Shandong University</strong>.</li>
<li><em>03.2024:</em> ğŸ‰ Our paper on Multimodal Sentiment Analysis has been accepted by <a href="https://www.google.com/search?q=ijcnn2024&oq=IJCNN&gs_lcrp=EgZjaHJvbWUqBggCECMYJzIGCAAQRRg9MgYIARBFGDsyBggCECMYJzIGCAMQABgeMgYIBBBFGDsyBggFEAAYHjIGCAYQRRg9MgYIBxBFGDzSAQg0MzIyajBqN6gCALACAA&sourceid=chrome&ie=UTF-8"><strong>IJCNN2024</strong></a>(CCF C).</li>
<li><em>01.2024:</em> ğŸ‰ Our paper "Faithful Vision-Language Interpretation via Concept Bottleneck Models" has been accepted at The 12th International Conference on Learning Representations <strong>(ICLR 2024)</strong>!.</li>
<li><em>10.2023:</em> ğŸ‰ Our paper "Multimodal sentiment analysis: A survey" has been accepted by the journal <a href="https://www.sciencedirect.com/journal/displays"><strong>Displays</strong></a> (JCR Q1).</li>
<li><em>10.2023:</em> ğŸ‰ Our paper "Cross-domain car detection model with integrated convolutional block attention mechanism" has been accepted by the journal <a href="https://www.sciencedirect.com/journal/image-and-vision-computing"><strong>Image and Vison Computing</strong></a> (JCR Q1; CCF C).</li>
<li><em>11.2022:</em> ğŸ‰Get the <strong>First Prize</strong> in Contemporary Undergraduate Mathematical Contest in Modeling National (top 0.6%).</li>
<li><em>11.2022:</em> ğŸ‰I am very glad to give an <strong>oral</strong> report at the international conference <a href="http://www.cisp-bmei.cn/">CISP-BMEI</a> 2022 and win the <strong>Best Paper Award</strong>.</li>
<li><em>10.2022:</em> ğŸ‰ Our paper "Predicting lysine phosphoglycerylation sites using bidirectional encoder representations with transformers & protein feature extraction and selection" has been accepted by CISP-BMEI 2022 (Tsinghua B)</li>
</ul>

</div>

<div class="section-divider"></div>

<span class='anchor' id='publications'></span>
# <span data-typewriter>ğŸ“ Publications (Selected)</span>

<div class="publications-scroll-container">

<h2>2026</h2>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2026</div><img src='images/ACE.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
<strong><a href="https://arxiv.org/pdf/2510.07896?">ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall</a></strong>

Jiayu Yangâ€ , Yuxuan Fanâ€ , <strong>Songning Laiâ€ </strong>, Shengen Wu, Jiaqi Tang, Chun Kang, Zhijiang Guo, Yutao Yue.

International Conference on Learning Representations <strong>ICLR2026</strong> <span class="venue-badge ccf-none">CCF None</span> <span class="venue-badge tier-top">Top Tier</span> <span class="venue-badge core-a-star">Core A*</span>.

<p class="paper-description">In this paper, we propose ACE, a knowledge editing framework based on neuron attribution control. By locating and correcting the key neuron pathways in the Transformer internal inference chain, ACE solves the problem of intermediate implicit subject failure when multi-hop knowledge updating in large language models, and reveals the cognitive mechanism of query neuron driving semantic accumulation.</p>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2026</div><img src='images/ICASSP26.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
<strong>[Towards Reliable Time Series Forecasting under Future Uncertainty: Ambiguity and Novelty Rejection Mechanisms]()</strong>

Ninghui Fengâ€ , <strong>Songning Laiâ€ </strong>, Xin Zhou, Jiayu Yang, Kunlong Feng, Zhenxiao Yin, Fobao Zhou, Zhangyi Hu, Yutao Yue, Yuxuan Liang, Boyu Wang, Hang Zhao

The Conference on <strong>ICASSP 2026</strong> <span class="venue-badge ccf-b">CCF B</span> <span class="venue-badge core-a">Core A</span>.

<p class="paper-description">We propose a dual rejection framework combining ambiguity rejection (using prediction error variance) and novelty rejection (leveraging VAEs and Mahalanobis distance) to enhance time series forecasting reliability by abstaining from low-confidence predictions and detecting distribution shifts, effectively reducing errors in dynamic environments. This approach addresses underfitting and out-of-distribution challenges without requiring future ground truth, advancing robust forecasting in complex real-world scenarios.</p>

</div>
</div>

<h2>2025</h2>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2025</div><img src='images/CONCIL.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
<strong><a href="https://arxiv.org/pdf/2411.17471?">Learning New Concepts, Remembering the Old: Continual Learning for Multimodal Concept Bottleneck Models</a></strong>

<strong>Songning Lai</strong>, Mingqian Liao, Zhangyi Hu, Jiayu Yang, Wenshuo Chen, Hongru Xiao, Jianheng Tang, Haicheng Liao, Yutao Yue~

The Conference on <strong>ACM MM 2025 BNI Track</strong> <span class="venue-badge ccf-a">CCF A</span> <span class="venue-badge tier-top">Top Tier</span> <span class="venue-badge core-a-star">Core A*</span> <span class="venue-badge oral">Oral</span> <span class="venue-badge outstanding">Outstanding</span>.

<p class="paper-description">This paper defines the continuous learning problem of CBM for the first time, and proposes a framework CONCIL to continuously learn concept and label simultaneously. Theoretical and experimental results verify the efficiency and absolute memory property of the framework.</p>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2025</div><img src='images/FTS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
<strong>[From Guesswork to Guarantee: Towards Faithful Multimedia Web Forecasting with TimeSieve]()</strong>

<strong>Songning Lai</strong>, Ninghui Feng, Jiechao Gao, Hao Wang, Haochen Sui, Xin Zou, Jiayu Yang, Wenshuo Chen, Lijie Hu, Hang Zhao, Xuming Hu, Yutao Yue

The Conference on <strong>ACM MM 2025</strong> <span class="venue-badge ccf-a">CCF A</span> <span class="venue-badge tier-top">Top Tier</span> <span class="venue-badge core-a-star">Core A*</span>.

<p class="paper-description">Study of time series forecasting models.</p>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2025</div><img src='images/WechatIMG169.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
<strong><a href="https://arxiv.org/pdf/2409.10330v1">DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving</a></strong>

<strong>Songning Lai</strong>, Tianlang Xue, Hongru Xiao, Lijie Hu, Jiemin Wu, Ruiqiang Xiao, Ninghui Feng, Haicheng Liao, Zhenning Yang, Yutao Yue~

The Conference on <strong>ICRA 2025</strong> <span class="venue-badge ccf-b">CCF B</span> <span class="venue-badge core-a-star">Core A*</span>.

<p class="paper-description">We introduce DRIVE, a framework designed to enhance the dependability and stability of explanations in end-to-end unsupervised autonomous driving models, addressing instability issues and improving trustworthiness through consistent and stable interpretability and output, as demonstrated by empirical evaluations. This framework provides novel metrics for assessing the reliability of concept-based explainable autonomous driving systems, advancing their real-world deployment.</p>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECML-PKDD 2025</div><img src='images/SVCT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
<strong><a href="https://arxiv.org/pdf/2506.05286?">Stable Vision Concept Transformers for Medical Diagnosis</a></strong>

Lijie Huâ€ , <strong>Songning Laiâ€ </strong>, Yuan Hua, Shu Yang, Jingfeng Zhang, Di Wang

The Conference on <strong>ECML-PKDD 2025</strong> <span class="venue-badge ccf-b">CCF B</span> <span class="venue-badge core-a">Core A</span>.

<p class="paper-description">The paper introduces Vision Concept Transformer (VCT) and its stable variant SVCT, which integrate vision transformers with concept features and denoised diffusion smoothing to preserve medical imaging accuracy while providing robust, interpretable explanations resilient to perturbations.</p>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2025</div><img src='images/WechatIMG168.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
<strong><a href="https://arxiv.org/pdf/2409.03192">PEPL: Precision-Enhanced Pseudo-Labeling for Fine-Grained Image Classification in Semi-Supervised Learning</a></strong>

Bowen Tianâ€ , <strong>Songning Laiâ€ </strong>, Lujundong Li, Zhihao Shuai, Runwei Guan, Tian Wu, Yutao Yue~

The Conference on <strong>ICASSP 2025</strong> <span class="venue-badge ccf-b">CCF B</span> <span class="venue-badge core-b">Core B</span>.

<p class="paper-description">We introduce Precision-Enhanced Pseudo-Labeling (PEPL), a semi-supervised learning approach for fine-grained image classification that generates and refines pseudo-labels using Class Activation Maps (CAMs) to capture essential details, significantly improving accuracy and robustness over existing methods on benchmark datasets. The approach consists of initial and semantic-mixed pseudo-label generation phases to enhance the quality of labels and has been open-sourced for public use.</p>

</div>
</div>



<h2>2024</h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/FVLC.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<strong><a href="https://openreview.net/forum?id=rp0EdI8X4e">Faithful Vision-Language Interpretation via Concept Bottleneck Models</a></strong>

<strong>Songning Lai</strong>, Lijie Hu, Junxiao Wang, Laure Berti and Di Wang

The Twelfth International Conference on Learning Representations <strong>ICLR2024</strong> <span class="venue-badge ccf-none">CCF None</span> <span class="venue-badge tier-top">Top Tier</span> <span class="venue-badge core-a-star">Core A*</span>.

<p class="paper-description">We introduce the Faithful Vision-Language Concept (FVLC) model, addressing the instability of label-free Concept Bottleneck Models (CBMs). Our FVLC model demonstrates superior stability against input and concept set perturbations across four benchmark datasets, with minimal accuracy degradation compared to standard CBMs, offering a reliable solution for model interpretation.</p>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/medicn.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
<strong><a href="https://arxiv.org/abs/2410.21494">Towards Multi-dimensional Explanation Alignment for Medical Classification</a></strong>

Lijie Huâ€ , <strong>Songning Laiâ€ </strong>, Wenshuo Chenâ€ , Hongru Xiao, Hongbin Lin, Lu Yu, Jingfeng Zhang, and Di Wang

The Conference on Neural Information Processing Systems <strong>NeurIPS 2024</strong> <span class="venue-badge ccf-a">CCF A</span> <span class="venue-badge tier-top">Top Tier</span> <span class="venue-badge core-a-star">Core A*</span>.

<p class="paper-description">We proposed an end-to-end framework called Med-MICN, which leverages the strength of different XAI methods such as concept-based models, neural symbolic methods, saliency maps, and concept semantics. Our outputs are interpreted in multiple dimensions, including concept prediction, saliency maps, and concept reasoning rules, making it easier for experts to identify and correct errors. Med-MICN demonstrates superior performance and interpretability compared with other concept-based models and the black-box model baselines.</p>  
</div>
</div>


</div>

<div class="section-divider"></div>

<span class='anchor' id='complete-publications'></span>
# ğŸ“š Complete Publications

<div class="publication-stats">
<strong>Publication Statistics:</strong> CCF A: 11 | ICLR: 2 | CCF B: 6 | CCF C: 6 | JCR Q1: 11
</div>

<div class="complete-publications-container">

<h2>2026</h2>
<ul class="publication-list">
<li>ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall, Jiayu Yangâ€ , Yuxuan Fanâ€ , <strong>Songning Laiâ€ </strong>, Shengen Wu, Jiaqi Tang, Chun Kang, Zhijiang Guo, Yutao Yue, <strong>ICLR 2026</strong> (CCF None).</li>
<li>TOWARDS RELIABLE TIME SERIESFORECASTING UNDER FUTURE UNCERTAINTY: AMBIGUITY AND NOVELTY REJECTION MECHANISMS, Ninghui Fengâ€ , <strong>Songning Laiâ€ </strong>, Xin Zou, ...,Hang Zhao, <strong>ICASSP 2026</strong> (CCF B).</li>
<li>TPTD: A Tursted Privacy-Preserving Truth Discovery Scheme for Quality Enhancement in Team-based Mobile Crowd Sensing, Yajiang Huang, ..., <strong>Songning Lai</strong>, ..., Houbing Herbert Song, <strong>Knowledge-Based Systems(KBS)</strong> (JCR Q1, IF: 7.2).</li>
</ul>



<h2>2025</h2>
<ul class="publication-list">
<li>Learning New Concepts, Remembering the Old: Continual Learning for Multimodal Concept Bottleneck Models, <strong>Songning Lai</strong>, Mingqian Liao, Zhangyi Hu, Jiayu Yang, Wenshuo Chen, Hongru Xiao, Jianheng Tang, Haicheng Liao, Yutao Yue, <strong>ACM MM 2025 Brave New Idea Track</strong> (CCF A, Core A*) <BNI Papers are considered outstanding ACM MM full papers, and accepted BNI papers will apear in the main proceedings>.</li>

<li>From Guesswork to Guarantee: Towards Faithful Multimedia Web Forecasting with TimeSieve, <strong>Songning Lai</strong>, Ninghui Feng, Jiechao Gao, Hao Wang, Haochen Sui, Xin Zou, Jiayu Yang, Wenshuo Chen, Hang Zhao, Xuming Hu, Yutao Yue, <strong>ACM MM 2025</strong> (CCF A, Core A*).</li>
<li>DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving, <strong>Songning Lai</strong>, Ninghui Feng, Jiechao Gao, Hao Wang, Haochen Sui, Xin Zou, Jiayu Yang, Wenshuo Chen, Hang Zhao, Xuming Hu, Yutao Yue, <strong>ICRA 2025</strong> (CCF B, Core A*).</li>
<li>Stable Vision Concept Transformers for Medical Diagnosis, Lijie Huâ€ , <strong>Songning Laiâ€ </strong>, Yuan Huaâ€ , Jingfeng Zhang, Pan Zhou, Di Wang, <strong>ECML-PKDD 2025</strong> (CCF B, Core A).</li>
<li>PEPL: Precision-Enhanced Pseudo-Labeling for Fine-Grained Image Classification in Semi-Supervised Learning, Bowen Tianâ€ , <strong>Songning Laiâ€ </strong>, Lujundong Li, Zhihao Shuai, Runwei Guan, Tian Wu, Yutao Yue, <strong>ICASSP 2025</strong> (CCF B, Core B).</li>
<li>IMTS is Worth Time X Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction, Zhangyi Hu, Jiemin Wu, Hua Xu, Minqian Liao, Ninghui Feng, Bo Gao, <strong>Songning Lai</strong>, Yutao Yue, <strong>ICML 2025</strong> (CCF A, Core A*).</li>
<li>Physics-Informed Representation Alignment for Sparse Radio-Map Reconstruction, Jia Haozhe, Wenshuo Chen, Huang Zhihui, Lei Wang, Hongru Xiao, Jia Nanqian, Keming Wu, <strong>Songning Lai</strong>, Bowen Tian, Yutao Yue, <strong>ACM MM 2025 Brave New Idea Track</strong> (CCF A, Core A*) <BNI Papers are considered outstanding ACM MM full papers, and accepted BNI papers will apear in the main proceedings>.</li>
<li>Can Audio Language Models Listen Between the Lines? A Study on Metaphorical Reasoning via Unspoken, Hongru Xiao, Xiang Li, Duyi Pan, Longfei Zhang, ZhixueSong, Jiale Han, <strong>Songning Lai</strong>, Wenshuo Chen, Jing Tang, Benyou Wang, <strong>ACM MM 2025 Brave New Idea Track</strong> (CCF A, Core A*) <BNI Papers are considered outstanding ACM MM full papers, and accepted BNI papers will apear in the main proceedings>.</li>
<li>IMTS is Worth Time X Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction, Zhangyi Hu, Jiemin Wu, Hua Xu, Minqian Liao, Ninghui Feng, Bo Gao, <strong>Songning Lai</strong>, Yutao Yue, <strong>ICML 2025</strong> (CCF A, Core A*).</li>
<li>ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model, Wenshuo Chen, Kuimou Yu, Jia Haozhe, Kaishen Yuan, Zexu Huang, Bowen Tian, <strong>Songning Lai</strong>, Hongru Xiao, Erhang Zhang, Lei Wang, Yutao Yue, <strong>ACM MM 2025</strong> (CCF A, Core A*).</li>
<li>Text2Weight: Bridging Natural Language and Neural Network Weight Spaces, Bowen Tian, Wenshuo Chen, Zexi Li, <strong>Songning Lai</strong>, Jiemin Wu, Yutao Yue, <strong>ACM MM 2025</strong> (CCF A, Core A*).</li>
<li>CFSSeg: Closed-Form Solution for Class-Incremental Semantic Segmentation of 2D Images and 3D Point Clouds, Jiaxu Li, Rui Li, Jianyu Qi, <strong>Songning Lai</strong>, Linpu Lv, Kejia Fan, Jianheng Tang, Yutao Yue, Dongzhan Zhou, Yunhuai Liu, Huiping Zhuang, <strong>ACM MM 2025</strong> (CCF A, Core A*).</li>
<li>Beyond Patterns: Harnessing Causal Logic for Autonomous Driving Trajectory Prediction, Bonan Wang, Haicheng Liao, Chengyue Wang, Bin Rao, Yanchen Guan, Guyang Yu, Jiaxun Zhang, <strong>Songning Lai</strong>, Chengzhong Xu, Zhenning Li, <strong>IJCAI 2025</strong> (CCF A, Core A*).</li>
<li>Boosting Expertise and Efficiencyin LLM:A Knowledge-Enhanced Framework for Construction Support, Bin Yang, Hongru Xiao, Zixuan Zenga, <strong>Songning Lai</strong>, Jiale Han, Yanke Tana and Yiqing Ni, <strong>Expert Systems With Applications</strong> (JCR Q1, IF:8.4, CCF C).</li>
<li>Boosting Expertise and Efficiencyin LLM:A Knowledge-Enhanced Framework for Construction Support, Hongru Xiao, ..., <strong>Songning Lai</strong>, <strong>Alexandria Engineering Journal</strong> (JCR Q1, IF: 6.8)!</li>
<li>Generative Knowledge-Guided Review System for Construction Disclosure Documents, Hongru Xiao, Jiankun Zhuanga, Bin Yanga, Jiale Hanb, Yantao Yu and <strong>Songning Lai</strong>, <strong>Advanced engineering informatics</strong> (JCR Q1, IF: 9.9, CCF B).</li>
<li>Automated Detection of Complex Construction Scenes Using a Lightweight Transformer-based Method, Hongru Xiao, Bin Yang, Yujie Lu, Wenshuo Chen, <strong>Songning Lai</strong>, Biaoli Gao, <strong>Automation in Construction</strong> (JCR Q1, IF:9.6).</li>
<li>Enhancing domain adaptation for plant diseases detection through Masked Image Consistency in Multi-Granularity Alignment, Guinan Guo, <strong>Songning Lai</strong>, Qingyang Wu, Yuntao Shou, Wenxu Shi, <strong>Expert Systems With Applications</strong> (JCR Q1, IF:8.4, CCF C).</li>
<li>Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding, Runwei Guan, ...., <strong>Songning Lai</strong>, ... ,Hui Xiong, IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY <strong>TCSVT</strong>. (IF: 11.1, JCR Q1, CCF B)</li>
</ul>


<h2>2024</h2>
<ul class="publication-list">
<li>Faithful Vision-Language Interpretation via Concept Bottleneck Models, <strong>Songning Lai</strong>, Lijie Hu, Junxiao Wang, Laure Berti and Di Wang, The Twelfth International Conference on Learning Representations <strong>ICLR2024</strong>(CCF None).</li>
<li>Towards Multi-dimensional Explanation Alignment for Medical Classification, Lijie Huâ€ , <strong>Songning Laiâ€ </strong>, Wenshuo Chenâ€ , Hongru Xiao, Hongbin Lin, Lu Yu, Jingfeng Zhang, and Di Wang, The Conference on Neural Information Processing Systems <strong>NeurIPS 2024</strong>(CCF A).</li>
<li>Shared and private information learning in multimodal sentiment analysis with deep modal alignment and self-supervised multi-task learning, <strong>Songning Laâ€ </strong>, Jiakang Li, Guinan Guo, Xifeng Hu, Yulong Li, Yuan Tan, Zichen Song, Yutong Liu, Zhaoxia Ren~, Chun Wang~, Danmin Miao~ and Zhi Liu~, International Joint Conference on Neural Networks <strong>IJCNN 2024</strong>(CCF C).</li>
<li>A Comprehensive Review of Community Detection in Graphs, Jiakang Liâ€ , <strong>Songning Laiâ€ </strong>, Zhihao Shuai, Yuan Tan, Yifan Jia, Mianyang Yu, Zichen Song, Xiaokang Peng, Ziyang Xu, Yongxin Ni, Haifeng Qiu, Jiayu Yang, Yutong Liu, Yonggang Lu~, <strong>Neurocomputing</strong> (JCR Q1 (IF: 6.0) CCF C).</li>
<li>Multimodal Sentiment Analysis: A Survey, <strong>Songning Lai</strong>, Haoxuan Xu, Xifeng Hu, Zhaoxia Ren~ and Zhi Liu~, <strong>Displays</strong> (JCR Q1 (IF: 4.3)).</li>
<li>Cross-domain car detection model with integrated convolutional block attention mechanism, Haoxuan Xuâ€ , <strong>Songning Laiâ€ </strong> and Yang Yang~, <strong>Image and Vision Computing</strong> (JCR Q1 (IF:4.7) CCF C).</li>
<li>Predicting Lysine Phosphoglycerylation Sites using Bidirectional Encoder Representations with Transformers & Protein Feature Extraction and Selection, <strong>Songning Lai</strong>, Xifeng Hu, Jing Han, Chun Wang, Subhas Mukhopadhyay, Zhi Liu~ and Lan Ye~, 2022 15th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics <strong>CISP-BMEI 2022</strong>(Tsinghua B).</li>
</ul>

</div>







<div class="section-divider"></div>

<span class='anchor' id='honors-and-awards'></span>
# ğŸ– Honors and Awards

<div class="honors-container">
<div class="honor-card">
<span class="honor-icon">ğŸ†</span>
<div class="honor-content">
<strong>NIPS 2024 Travel Awards</strong>
<span class="honor-badge">Conference Award</span>
</div>
</div>

<div class="honor-card">
<span class="honor-icon">ğŸ†</span>
<div class="honor-content">
<strong>ICRA 2025 Travel Awards</strong>
<span class="honor-badge">Conference Award</span>
</div>
</div>

<div class="honor-card highlight">
<span class="honor-icon">â­</span>
<div class="honor-content">
<strong>IEEE/EI (CISP-BMEI 2022) Best Paper Award</strong>
<span class="honor-badge best-paper">Best Paper</span>
</div>
</div>

<div class="honor-card highlight">
<span class="honor-icon">ğŸ¥‡</span>
<div class="honor-content">
<strong>First Prize in Contemporary Undergraduate Mathematical Contest in Modeling National</strong>
<span class="honor-badge top-percent">Top 0.6%</span>
</div>
</div>

<div class="honor-card highlight">
<span class="honor-icon">ğŸ¥‡</span>
<div class="honor-content">
<strong>First Prize in MathorCup University Mathematical Modeling Challenge National</strong>
<span class="honor-badge top-percent">Top 3%</span>
</div>
</div>

<div class="honor-card">
<span class="honor-icon">ğŸ¥ˆ</span>
<div class="honor-content">
<strong>Second Prize in National Undergraduate Electronic Design Contest (Shandong Province)</strong>
<span class="honor-badge">Provincial</span>
</div>
</div>

<div class="honor-card">
<span class="honor-icon">ğŸ¥ˆ</span>
<div class="honor-content">
<strong>Second Prize in National Crypto-math Challenge (East China Competition)</strong>
<span class="honor-badge">Regional</span>
</div>
</div>

<div class="honor-card">
<span class="honor-icon">ğŸ“</span>
<div class="honor-content">
<strong>Outstanding graduates of Shandong Province</strong>
<span class="honor-badge">Provincial</span>
</div>
</div>

<div class="honor-card">
<span class="honor-icon">ğŸ“</span>
<div class="honor-content">
<strong>Outstanding graduate of Shandong University</strong>
<span class="honor-badge">University</span>
</div>
</div>

<div class="honor-card stats">
<span class="honor-icon">ğŸ“Š</span>
<div class="honor-content">
<strong>More than 40 university-level awards</strong>
<span class="honor-badge">Multiple Categories</span>
<p class="honor-detail">Including academic competition, social practice, innovation and entrepreneurship, sports, aesthetic education, volunteer, scholarship and other aspects</p>
</div>
</div>
</div>


<div class="section-divider"></div>

<span class='anchor' id='educations'></span>
# ğŸ“– Educations and Experiences

<div class="timeline-container">
<div class="timeline-item">
<div class="timeline-icon">ğŸ”¬</div>
<div class="timeline-content">
<div class="timeline-date">Apr 2024 - Sep 2025</div>
<div class="timeline-title">HKUST(GZ) - Research Assistant</div>
<div class="timeline-desc">AI Thrust & INFO Hub</div>
</div>
</div>

<div class="timeline-item">
<div class="timeline-icon">ğŸŒ</div>
<div class="timeline-content">
<div class="timeline-date">Apr 2023 - Mar 2024</div>
<div class="timeline-title">KAUST - Visiting Student</div>
<div class="timeline-desc">International Research Experience</div>
</div>
</div>

<div class="timeline-item">
<div class="timeline-icon">ğŸ“</div>
<div class="timeline-content">
<div class="timeline-date">Sep 2020 - June 2024</div>
<div class="timeline-title">Shandong University - Bachelor of Science</div>
<div class="timeline-desc">School of Information Science and Engineering (EECS)</div>
</div>
</div>
</div>


<div class="section-divider"></div>

<span class='anchor' id='internships'></span>
# ğŸ’» Internships

<div class="internship-container">
<div class="internship-card">
<span class="internship-icon">ğŸ“</span>
<div class="internship-content">
<strong>Conference & Journal Reviewer</strong>
<div class="internship-tags">
<span class="tag">ECAI2024</span>
<span class="tag">ICML2024</span>
<span class="tag">KDD2024</span>
<span class="tag">ICLR2025</span>
<span class="tag">CVPR2025</span>
<span class="tag">ICCV2025</span>
<span class="tag">NIPS2025</span>
<span class="tag">ACM MM 2025</span>
<span class="tag">IJCAI2025</span>
<span class="tag">Expert Systems</span>
<span class="tag">+ More</span>
</div>
</div>
</div>

<div class="internship-card">
<span class="internship-icon">ğŸ‘¨â€ğŸ’¼</span>
<div class="internship-content">
<strong>Monitor of Chongxin College</strong>
<div class="internship-badges">
<span class="internship-badge highlight">Shandong Provincial Excellent Class</span>
<span class="internship-badge highlight">Shandong University Top Ten Class</span>
</div>
</div>
</div>

<div class="internship-card">
<span class="internship-icon">â¤ï¸</span>
<div class="internship-content">
<strong>Outstanding Volunteer</strong>
<div class="internship-stats">
<span class="stat-badge">130h</span> Total Volunteer Time
</div>
</div>
</div>
</div>

<div class="section-divider"></div>

# ğŸ§‘â€ğŸ¤â€ğŸ§‘ <span data-i18n="sections.friends">Friends</span>

<div class="friends-container">
<div class="friend-card">
<div class="friend-avatar">ğŸ‘¨â€ğŸ’»</div>
<div class="friend-info">
<div class="friend-name"><a href="https://yjywdzh.github.io/">Jiayu Yang</a></div>
<div class="friend-desc">Research Collaborator</div>
</div>
</div>

<div class="friend-card">
<div class="friend-avatar">ğŸ‘¨â€ğŸ’»</div>
<div class="friend-info">
<div class="friend-name"><a href="https://chatonz.github.io">Wenshuo Chen</a></div>
<div class="friend-desc">Research Collaborator</div>
</div>
</div>

<div class="friend-card">
<div class="friend-avatar">ğŸ‘¨â€ğŸ’»</div>
<div class="friend-info">
<div class="friend-name"><a href="https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://scholar.google.com.hk/citations%3Fuser%3DINre9KUAAAAJ%26hl%3Den&ved=2ahUKEwjax_-Wi62QAxXrslYBHf-_FkYQFnoECBYQAQ&usg=AOvVaw1jkqmPpNIt9Xn1q4vXBmPX">Jiemin Wu</a></div>
<div class="friend-desc">Research Collaborator</div>
</div>
</div>

<div class="friend-card">
<div class="friend-avatar">ğŸ‘¨â€ğŸ’»</div>
<div class="friend-info">
<div class="friend-name"><a href="https://rwlinno.github.io">Weilin Ruan</a></div>
<div class="friend-desc">Research Collaborator</div>
</div>
</div>

<div class="friend-card">
<div class="friend-avatar">ğŸ‘¨â€ğŸ’»</div>
<div class="friend-info">
<div class="friend-name"><a href="https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://scholar.google.com/citations%3Fuser%3DpD_SnqMAAAAJ%26hl%3Dzh-CN&ved=2ahUKEwidqfWsj62QAxVsZvUHHf9qKQQQFnoECB8QAQ&usg=AOvVaw0FEvdkonT5WHbpJD9UimpQ">Xinmin Li</a></div>
<div class="friend-desc">Research Collaborator</div>
</div>
</div>

<div class="friend-card">
<div class="friend-avatar">ğŸ‘¨â€ğŸ’»</div>
<div class="friend-info">
<div class="friend-name"><a href="https://jiakanglee.github.io">Jiakang Li</a></div>
<div class="friend-desc">Research Collaborator</div>
</div>
</div>
</div>

---

<span class='anchor' id='guestbook'></span>
## ğŸ’¬ ç•™è¨€æ¿ / Guestbook

<div class="like-section">
  <div class="like-counter">
    <button id="like-btn" class="like-btn" aria-label="ç‚¹èµ">
      <span class="like-icon">â¤ï¸</span>
      <span class="like-text">ç‚¹èµ</span>
      <span class="like-count" id="like-count">0</span>
    </button>
    <div class="like-message" id="like-message"></div>
  </div>
</div>

<div class="guestbook-intro">
  <p>æ¬¢è¿ç•™ä¸‹ä½ çš„æƒ³æ³•ã€å»ºè®®æˆ–ç¥ç¦ï¼ä½ å¯ä»¥ï¼š</p>
  <ul>
    <li>ğŸ’¬ åˆ†äº«ä½ çš„æƒ³æ³•å’Œåé¦ˆ</li>
    <li>ğŸ‰ ç•™ä¸‹ç¥ç¦å’Œé¼“åŠ±</li>
    <li>ğŸ¤ æå‡ºåˆä½œæˆ–äº¤æµå»ºè®®</li>
    <li>ğŸ“ åˆ†äº«ä½ çš„å­¦æœ¯è§è§£</li>
  </ul>
  <p><strong>æ³¨æ„</strong>ï¼šç•™è¨€åŠŸèƒ½éœ€è¦ GitHub è´¦å·ç™»å½•ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰å¯ç”¨ GitHub Discussionsï¼Œè¯·æŒ‰ç…§ä¸‹é¢çš„è¯´æ˜è¿›è¡Œé…ç½®ã€‚</p>
</div>

<div class="giscus-container">
  <script src="https://giscus.app/client.js"
    data-repo="xll0328/xll0328.github.io"
    data-repo-id="R_kgDONIgIgQ"
    data-category="Announcements"
    data-category-id="DIC_kwDONIgIgc4C1r_r"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="1"
    data-input-position="top"
    data-theme="preferred_color_scheme"
    data-lang="zh-CN"
    data-loading="lazy"
    crossorigin="anonymous"
    async>
  </script>
</div>

