---
permalink: /fvlc/
title: "FVLC: Faithful Vision-Language Interpretation via Concept Bottleneck Models"
excerpt: "ICLR 2024 — Official project page and code"
author_profile: true
---

**This paper was accepted at ICLR 2024** (The Twelfth International Conference on Learning Representations). [**Paper (OpenReview)**](https://openreview.net/pdf?id=rp0EdI8X4e) · [**Code (GitHub)**](https://github.com/xll0328/FVLC)

**Authors:** Songning Lai, Lijie Hu, Junxiao Wang, Laure Berti-Equille, Di Wang

---

## Overview figure (from paper)

<img src="{{ '/images/fvlc/overview.jpg' | relative_url }}" alt="Label-free CBM overview" style="max-width: 95%; height: auto;" />

**Label-free CBM pipeline.** FVLC adds faithfulness losses (L₂–L₄) and PGD-based perturbations to this pipeline to achieve stable concept and prediction behavior.

---

## Abstract

The demand for transparency in healthcare and finance has led to interpretable machine learning (IML) models, notably **Concept Bottleneck Models (CBMs)**, valued for their potential in performance and insights into deep neural networks. However, CBM's reliance on manually annotated data poses challenges. **Label-free CBMs** have emerged to address this, but they remain **unstable**, affecting their faithfulness as explanatory tools.

We introduce a formal definition for **Faithful Vision-Language Concept (FVLC)** and a methodology for constructing an FVLC that satisfies four critical properties. Our experiments on four benchmark datasets (CIFAR-10, CIFAR-100, CUB, Places365) demonstrate that FVLC outperforms baselines regarding stability against **input** and **concept set** perturbations (WP1, WP2, IP), with minimal accuracy degradation compared to vanilla Label-free CBM.

---

## Four properties of a faithful concept

Within the same concept set (e.g. generated by an LLM for the classes):

1. **Similarity of explanation** — Significant overlap between the top-*k* indices of the faithful concept and the original concept (interpretability).
2. **Stability of explanation** — The concept vector remains robust against random noise and perturbations in the concept set (e.g. word perturbations).
3. **Closeness of prediction** — The prediction distribution is close to that of the vanilla CBM (preserve performance).
4. **Stability of prediction** — The output distribution remains stable under perturbations (e.g. input noise, concept-set changes).

---

## Method: Min-max objective (Eq. 7)

We optimize a min-max objective with four loss terms **L**₁–**L**₄:

- **L**₁: Top-*k* overlap loss (explanation similarity).
- **L**₂: Concept stability under perturbations (δ on input, ρ on concept words).
- **L**₃: Prediction closeness to vanilla CBM.
- **L**₄: Output stability under perturbations.

**PGD** is used to find worst-case perturbations δ (input) and ρ (concept embeddings); then the projection layer (and optionally the final layer) is updated by gradient descent. See the paper for Algorithm 1 and surrogate loss details.

---

## Perturbations and metrics

- **WP1:** Word perturbation — replace a fraction (5% or 10%) of concept words with random words from the vocabulary.
- **WP2:** Word embedding perturbation — add Gaussian noise (e.g. σ=0.001) to concept embeddings.
- **IP:** Input perturbation — add bounded noise to the input image.

**Metrics:**

- **TCPC** (Top Concept Prediction Change): measures change in top concept indices under perturbation; lower is more stable.
- **TOPC** (Top Overlap Prediction Change): measures change in top-overlap prediction; lower is more stable.

---

## Main results (from paper)

### Table 1: Accuracy (%)

| Method | CIFAR10 | CIFAR100 | CUB | Places365 |
|--------|---------|----------|-----|-----------|
| Standard (no interpretability) | 88.80 | 70.10 | 76.70 | 48.56 |
| P-CBM (CLIP) | 84.50 | 56.00 | N/A | N/A |
| Label-free CBM | 86.32 | 65.42 | 74.23 | 43.63 |
| WP1(10%) – base | 86.25 | 65.09 | 73.97 | 43.67 |
| **WP1(10%) – FVLC** | **86.39** | 64.90 | 73.92 | 43.62 |
| WP2 – base | 86.41 | 65.16 | 73.96 | 43.54 |
| **WP2 – FVLC** | 86.22 | **65.34** | **74.44** | **44.55** |
| IP – base | 86.62 | 65.36 | 74.39 | 43.64 |
| **IP – FVLC** | **86.88** | 65.29 | 74.01 | 43.71 |

FVLC maintains or improves accuracy while greatly improving stability.

### Table 2: Stability (TCPC / TOPC) — lower is better

| Method | CIFAR10 | CIFAR100 | CUB | Places365 |
|--------|---------|----------|-----|-----------|
| WP1(10%) – base | 1.99E-01 / 8.36E-02 | 1.94E-01 / 1.31E-01 | 2.32E-01 / 3.41E-01 | 2.26E-01 / 1.14E-01 |
| **WP1(10%) – FVLC** | **1.19E-03 / 7.40E-03** | **3.67E-03 / 4.55E-03** | **1.19E-02 / 1.53E-03** | **1.39E-03 / 1.25E-03** |
| WP2 – base | 1.53E-01 / 4.99E-02 | 1.36E-01 / 6.67E-02 | 1.43E-01 / 1.73E-01 | 1.40E-01 / 6.37E-02 |
| **WP2 – FVLC** | **1.10E-02 / 8.72E-03** | **3.35E-03 / 4.55E-03** | **1.05E-02 / 1.53E-03** | **1.55E-03 / 1.29E-03** |
| IP – base | 1.68E-01 / 6.28E-02 | 1.38E-01 / 8.81E-02 | 1.71E-01 / 2.23E-01 | 1.73E-01 / 8.09E-02 |
| **IP – FVLC** | **8.02E-03 / 8.29E-03** | **3.24E-03 / 4.56E-03** | **1.04E-02 / 1.59E-03** | **1.50E-03 / 1.25E-03** |

### Ablation (Table 3)

Ablation over **L**₂, **L**₃, **L**₄ shows that all three terms contribute; using all three (✓✓✓) yields the best TCPC/TOPC across WP1(10%), WP2, and IP settings.

---

## Illustration: individual decision (from paper)

<img src="{{ '/images/fvlc/decision.png' | relative_url }}" alt="Individual decision explanation" style="max-width: 95%; height: auto;" />

**Example of concept-based explanation.** FVLC makes such explanations stable under input and concept-set perturbations.

---

## Code and data

- **Repository:** [https://github.com/xll0328/FVLC](https://github.com/xll0328/FVLC)
- **Datasets:** CIFAR-10, CIFAR-100, CUB-200-2011, Places365 (concept sets under `data/concept_sets/`).
- **Training:** `train_cbm.py` (base Label-free CBM), `train_fcbm_all.py` (full FVLC), `train_fcbm_projonly.py` (projection-only FVLC). See README in the repo for setup and commands.

---

## Citation

```bibtex
@inproceedings{lai2023faithful,
  title={Faithful Vision-Language Interpretation via Concept Bottleneck Models},
  author={Lai, Songning and Hu, Lijie and Wang, Junxiao and Berti-Equille, Laure and Wang, Di},
  booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
  year={2024}
}
```
